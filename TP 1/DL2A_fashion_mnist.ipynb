{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab session in an introduction to feed-forward neural network with pytorch. We use the dataset Fashion-MNIST (see for more details this website https://github.com/zalandoresearch/fashion-mnist). The dataset contains 60000 and 10000 images for respectively training and testing. Each image is 28x28 pixels, for a total of 784 per image.  An image is presented to the neural network as a flat vector of 784 component. \n",
    "\n",
    "\n",
    "In this lab session, you will experiment different kind of feed-forward networks, starting with simple models,  and then increasing their complexity. \n",
    "\n",
    "First load and test python and pytorch. Your notebook is supposed to work with python 3 (see the top right corner of the notebook).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats=['svg']\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "print(th.__version__) # should be greater or equal to 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "To simplify, just download and / or read the picke file provided. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "fp = gzip.open('fashion-mnist.pk.gz','rb')\n",
    "allXtrain, allYtrain, Xtest, Ytest, classlist  = pickle.load(fp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: The dataset is split in two parts, the training set and the test set.\n",
    "For thorough study and evaluation of machine learning models, a good practice is to consider the data in 3 parts: \n",
    "- the **training** set to learn the model parameters;\n",
    "- the **validation** set to tune the hyper parameters and some design choices (the number and the size of the hidden layers, the dropout probability, ...);\n",
    "- the **test** set to evaluate the model at the end. \n",
    "\n",
    "\n",
    "For the moment, we leave the test set and focus on the training set. \n",
    "To spare time, we will only consider the first 20000 images for training in the following set of experiments. And we also build a validation set to compare the results we obtain with different hyper-parameters. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   shape: torch.Size([20000, 784])\n",
      "Validation shape: torch.Size([10000, 784])\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Ytrain  = allXtrain[:20000], allYtrain[:20000]\n",
    "Xvalid, Yvalid  = allXtrain[20000:30000], allYtrain[20000:30000]\n",
    "print(\"Training   shape:\" ,Xtrain.shape)\n",
    "print(\"Validation shape:\" ,Xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the dataset explore the training set made of Xtrain and Ytrain. \n",
    "- Look at the dimension and type of the tensors\n",
    "- Print also the classlist variable. \n",
    "- Then look at some example to check consistency. \n",
    "\n",
    "For that purpose you can plot an image like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1deda89aad0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"300.237pt\" height=\"297.190125pt\" viewBox=\"0 0 300.237 297.190125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-03-31T08:52:44.132702</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.6.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 297.190125 \nL 300.237 297.190125 \nL 300.237 0 \nL -0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 273.312 \nL 293.037 273.312 \nL 293.037 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#pff52105a9e)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"image6b509f573c\" transform=\"scale(1 -1) translate(0 -266.4)\" x=\"26.925\" y=\"-6.912\" width=\"266.4\" height=\"266.4\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m12321f60ee\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m12321f60ee\" x=\"31.677\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(28.49575 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m12321f60ee\" x=\"79.197\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(76.01575 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m12321f60ee\" x=\"126.717\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(120.3545 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m12321f60ee\" x=\"174.237\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(167.8745 287.910437) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m12321f60ee\" x=\"221.757\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(215.3945 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m12321f60ee\" x=\"269.277\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(262.9145 287.910437) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"m4ce6ae95d1\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m4ce6ae95d1\" x=\"26.925\" y=\"11.952\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 15.751219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m4ce6ae95d1\" x=\"26.925\" y=\"59.472\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 63.271219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m4ce6ae95d1\" x=\"26.925\" y=\"106.992\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 110.791219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m4ce6ae95d1\" x=\"26.925\" y=\"154.512\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 158.311219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m4ce6ae95d1\" x=\"26.925\" y=\"202.032\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 205.831219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m4ce6ae95d1\" x=\"26.925\" y=\"249.552\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 253.351219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 273.312 \nL 26.925 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 293.037 273.312 \nL 293.037 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 273.312 \nL 293.037 273.312 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 293.037 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pff52105a9e\">\n   <rect x=\"26.925\" y=\"7.2\" width=\"266.112\" height=\"266.112\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Xtrain[1].numpy().reshape(28,28) , matplotlib.pyplot.cm.gray)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first overview of the dataset, we can do a PCA (in 2D) of the training set. The following image represents the result: \n",
    "\n",
    "<img src=\"https://allauzen.github.io/assets/figs/pca-fashion-10-classes.png\" \n",
    "    style=\"width:400px; margin:0px auto;display:block\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward neural network\n",
    "\n",
    "\n",
    "\n",
    "A feedforward model can rely on the pytorch module *Sequential*. \n",
    "A *Sequential* module is a container of ordered modules: the first one takes input data and its output is given to feed the second module, and so on. \n",
    "\n",
    "**Note: ** In pytorch, modules assumed Tensors as input. The input Tensor can contain just one input (online mode) or several inputs (mini-batch). The first dimension of the input Tensor corresponds to the mini-batch, and the second one to the dimension of the example to feed in. For example, with a mini-batch of size B and an image of D pixels, the input Tensor should be of shape (B,D), even if B=1 for one exemple. \n",
    "\n",
    "\n",
    "## Shallow network\n",
    "\n",
    "Let start with a simple model with one input layer and one output layer (without hidden layers). Please refer to the examples provided previously, and propose an implementation of this linear model using the *Sequential* module as container.  To write the model, we must consider the fact that the model is trained in order to maximize the Log-likelihood on the training data. If you look at  https://pytorch.org/docs/stable/nn.html, the documentation of the NNet package of pytorch, there is a section on the loss functions. \n",
    "\n",
    "Two of the proposed loss function can be used for our purpose. The choice of one of them implies the choice of the activation function at the output layer. \n",
    "\n",
    "- What are these two possible choices ? \n",
    "\n",
    "\n",
    "Make a choice and replace the \"None\" in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = 784\n",
    "D_out= 10\n",
    "\n",
    "## TODO : replace \"None\"\n",
    "model =  None \n",
    "loss_function = None \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then test the code on a minibatch of B examples. The code below corresponds to a prediction on a single image and then on 3 images. Look at the results, their shapes and values. Is it consistent with what you expect ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=1\n",
    "i = 0\n",
    "pred = model(Xtrain[i:i+B])\n",
    "# explore \n",
    "print(pred)\n",
    "B=3\n",
    "i = 0\n",
    "pred = model(Xtrain[i:i+B])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same with the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=1\n",
    "i = 1\n",
    "pred = model(Xtrain[i:i+B])\n",
    "loss = loss_function(pred, Ytrain[i:i+B])\n",
    "print(loss)\n",
    "# explore \n",
    "B=3\n",
    "i = 1\n",
    "pred = model(Xtrain[i:i+B])\n",
    "loss = loss_function(pred, Ytrain[i:i+B])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online learning  and mini-batch\n",
    "\n",
    "We will start by writing the code for training a model. The code should be as generic as possible to handle different kind of model. Afterwards, you will wrap your code in a function. \n",
    "\n",
    "We will use the Adam optimizer with an initial learning rate of 0.001.  \n",
    "\n",
    "The following cell contains examples of code that can be useful to train the model:  \n",
    "- Init. of the model and the optimizer.  \n",
    "- Ideas the iterate over the dataset in a random order\n",
    "- Mini-batch processing \n",
    "- Forward and backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The model, the loss and optimizer\n",
    "model=nn.Sequential(nn.Linear(D_in,D_out),\n",
    "                    nn.LogSoftmax(dim=1))\n",
    "optimizer=th.optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_fn=nn.NLLLoss()\n",
    "\n",
    "### Mini-batching and shuffle \n",
    "Ntrain = Xtrain.shape[0] \n",
    "Nvalid = Xvalid.shape[0]\n",
    "print(Ntrain,Nvalid)\n",
    "idx = np.arange(Ntrain) # Generate the indices \n",
    "batch_size = 200\n",
    "nbatch = int(Ntrain/batch_size) # the number of batches\n",
    "print(batch_size, nbatch)\n",
    "\n",
    "np.random.shuffle(idx) # random order of indices \n",
    "bi = 2 # a random choice\n",
    "ids = idx[bi*batch_size:(bi+1)*batch_size] # Get indices\n",
    "images = Xtrain[ids]  # Get the images\n",
    "labels = Ytrain[ids]  # The Labels\n",
    "optimizer.zero_grad()\n",
    "logprobs=model(images) # inference \n",
    "loss=loss_fn(logprobs,labels) # compute the loss\n",
    "print(\"Loss function: \",loss)     \n",
    "loss.backward() # Back propagation\n",
    "optimizer.step() # update the parameters\n",
    "# Note: just for illustration and checking, we can recompute \n",
    "# the loss on the same minibatch, but after the update. \n",
    "logprobs=model(images) # inference \n",
    "loss=loss_fn(logprobs,labels) # compute the loss\n",
    "print(\"Loss function: \",loss.item()) # better print ! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the previous example, write the \"full\" training: \n",
    "- Init. of everything useful\n",
    "- The training loop, \n",
    "- ... \n",
    "\n",
    "Remember that we need the values of the loss function both on the train and on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nepochs=30\n",
    "\n",
    "### The model, the loss and optimizer\n",
    "model=nn.Sequential(nn.Linear(D_in,D_out),\n",
    "                    nn.LogSoftmax(dim=1))\n",
    "optimizer=th.optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_fn=nn.NLLLoss()\n",
    "## TODO:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training loop with a batch size of 1 and then of size 200. The difference in terms of computation time should be significant ! We will now only use a batch size of 200. \n",
    "\n",
    "Note that in practice, the learning rate should be adapted to the mini-batch size. \n",
    "\n",
    "Run the previous training code with a batch size of 200 for 30 epochs. We can plot at the results like this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (9, 1.5))\n",
    "ax= plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax= plt.subplot(1, 3, 2)\n",
    "plt.plot(valid_losses)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax= plt.subplot(1, 3, 3)\n",
    "plt.plot(valid_accuracies)\n",
    "ax.set_ylim(bottom=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training wrapper \n",
    "\n",
    "Write a function that wraps what we need to train a model and look at the results. Test it with a new model. \n",
    "\n",
    "Good to notice : the call of  *model(X)* return a 2D tensor. The 2D tensor has a line for every image of the batch. The line of an image has one column per label (here 10). The tensor contains the log-probabilities that the image corresponds to the label of the\n",
    "column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=30,verbose=False):\n",
    "    # The verbose allows you to skip printed information per epoch.\n",
    "    # The function will only print the best accuracy on the validation\n",
    "    # and plot the learning curves. \n",
    "    print(\"TODO\")\n",
    "\n",
    "# When we create the model, its parameters are initialized. \n",
    "model=nn.Sequential(nn.Linear(D_in,D_out),\n",
    "                    nn.LogSoftmax(dim=1))\n",
    "optimizer=th.optim.Adam(model.parameters(),lr=0.001)\n",
    "# Note this important to build a new optimizer \n",
    "# if we want to have the reference to parameters \n",
    "# of the new model ! \n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with one hidden layer\n",
    "\n",
    "Now we have a function to train and evaluate the training process of a neural model, we can explore different configurations. Let start with a neural network with one hidden layer and a Sigmoid activation function on this hidden layer. We set the size of this hidden layer to 50. \n",
    "\n",
    "Write the model using the Sequential module, and train it: \n",
    "- for 30 epochs and with lr=0.001 and lr=0.0001\n",
    "- do the same and raise the number of epoch to 50\n",
    "What do you observe ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO \n",
    "hidden_layer = 50 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Sigmoid to ReLU\n",
    "\n",
    "Consider lr=0.0001 and train a similar model with a ReLU activation. Compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of the hidden layer size\n",
    "\n",
    "Run experiments with different hidden layer size, respectively : 50,100,150, 200 and 250. \n",
    "What do you observe ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper network\n",
    "\n",
    "Now we add one more hidden layer and consider a NNet with two hidden layers. \n",
    "The first setup is: \n",
    "- two hidden layers of size 50 with a ReLU activation\n",
    "- a learning rate of 0.0001\n",
    "\n",
    "Train it during 100 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different value of dropout to assess its impact on the training process. For example 0.3 and 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can play with different  \"hyper-parameters\":\n",
    "- Increase the size (double for example) of the first hidden layer\n",
    "- Add a third hidden layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix\n",
    "\n",
    "For a good model you obtained, compute the confusion matrix and look at it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Dropout \n",
    "\n",
    "You should observe overfitting. One solution is to add a dropout layer to the model (with a probability of 0.2 for example). Code this modification and rerun the training process to observe the impact. When you use a Dropout layer, the layer acts differently in the train mode and evaluation mode. You should take this into account when you train the model end when you compute the performance on the validation set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA \n",
    "\n",
    "To analyse the results, beyond the confusion matrix, we can compute the PCA (in 2D) and plot the projected datapoints depending on their classes, for instance by considering every pairs of classes. To compute the PCA and project the data, we can use the implementation provided by sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the PCA in 2D\n",
    "from sklearn.decomposition import PCA\n",
    "#  .... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test \n",
    "- Take two best models (with one and two hidden layers) and run the evaluation on the test set. \n",
    "- Train the best two models with all the training data and compute the results on the test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
